# app/pages/2_L√†m_s·∫°ch_&_Chu·∫©n_b·ªã.py ‚Äì B·∫¢N HO√ÄN CH·ªàNH NH·∫§T, C√ì MINH H·ªåA TR·ª∞C QUAN
import streamlit as st
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
import matplotlib.pyplot as plt

st.set_page_config(layout="wide")
st.header("üßπ 2. L√ÄM S·∫†CH & CHU·∫®N B·ªä D·ªÆ LI·ªÜU")

# ========== T·∫¢I D·ªÆ LI·ªÜU G·ªêC V√Ä ƒê√É X·ª¨ L√ù ==========
@st.cache_data
def load_data():
    # Gi·∫£ s·ª≠ ch√∫ng ta c√≥ c·∫£ data g·ªëc v√† data ƒë√£ x·ª≠ l√Ω
    try:
        movies_original = pd.read_csv("data/movies.csv")  # D·ªØ li·ªáu g·ªëc
    except:
        # T·∫°o d·ªØ li·ªáu m·∫´u n·∫øu kh√¥ng c√≥ file g·ªëc
        movies_original = pd.read_csv("data/movies_final.csv")
        # Th√™m m·ªôt s·ªë v·∫•n ƒë·ªÅ ƒë·ªÉ minh h·ªça
        movies_original_copy = movies_original.copy()
        movies_original_copy.loc[0:10, 'year'] = np.nan  # Missing values
        movies_original_copy = pd.concat([movies_original_copy, movies_original_copy.head(5)])  # Duplicate
        movies_original = movies_original_copy
    
    movies_cleaned = pd.read_csv("data/movies_final.csv")  # D·ªØ li·ªáu ƒë√£ l√†m s·∫°ch
    return movies_original, movies_cleaned

movies_orig, movies_clean = load_data()

# ========== TI√äU ƒê·ªÄ ·∫§N T∆Ø·ª¢NG ==========
st.markdown("""
<div style="background:linear-gradient(90deg, #059669, #10B981); padding:20px; border-radius:10px; color:white;">
    <h2 style="text-align:center; margin:0;">üßº QUY TR√åNH L√ÄM S·∫†CH D·ªÆ LI·ªÜU 5 B∆Ø·ªöC</h2>
</div>
""", unsafe_allow_html=True)

# ========== SO S√ÅNH TR∆Ø·ªöC - SAU ==========
st.markdown("---")
st.markdown("### üìä SO S√ÅNH D·ªÆ LI·ªÜU TR∆Ø·ªöC & SAU KHI L√ÄM S·∫†CH")

col_before, col_after = st.columns(2)

with col_before:
    st.markdown("#### üö® D·ªÆ LI·ªÜU G·ªêC (C√ì V·∫§N ƒê·ªÄ)")
    
    # Hi·ªÉn th·ªã s·ªë li·ªáu th·ªëng k√™
    metrics_before = st.columns(3)
    with metrics_before[0]:
        st.metric("Missing values", f"{movies_orig['year'].isnull().sum()}")
    with metrics_before[1]:
        st.metric("Duplicate", f"{movies_orig.duplicated(subset='movieId').sum()}")
    with metrics_before[2]:
        st.metric("S·ªë d√≤ng", f"{len(movies_orig):,}")
    
    # Hi·ªÉn th·ªã sample data g·ªëc
    with st.expander("üëÄ Xem d·ªØ li·ªáu g·ªëc (c√≥ v·∫•n ƒë·ªÅ)"):
        st.dataframe(movies_orig.head(10), use_container_width=True)

with col_after:
    st.markdown("#### ‚úÖ D·ªÆ LI·ªÜU ƒê√É L√ÄM S·∫†CH")
    
    # Hi·ªÉn th·ªã s·ªë li·ªáu th·ªëng k√™
    metrics_after = st.columns(3)
    with metrics_after[0]:
        st.metric("Missing values", f"{movies_clean['year'].isnull().sum()}", delta="0", delta_color="off")
    with metrics_after[1]:
        st.metric("Duplicate", f"{movies_clean.duplicated(subset='movieId').sum()}", delta="0", delta_color="off")
    with metrics_after[2]:
        st.metric("S·ªë d√≤ng", f"{len(movies_clean):,}", delta=f"-{len(movies_orig)-len(movies_clean)}")
    
    # Hi·ªÉn th·ªã sample data ƒë√£ l√†m s·∫°ch
    with st.expander("üëÄ Xem d·ªØ li·ªáu ƒë√£ l√†m s·∫°ch"):
        st.dataframe(movies_clean.head(10), use_container_width=True)

# ========== 5 B∆Ø·ªöC L√ÄM S·∫†CH CHI TI·∫æT ==========
st.markdown("---")
st.markdown("### üõ†Ô∏è CHI TI·∫æT 5 B∆Ø·ªöC L√ÄM S·∫†CH (V∆Ø·ª¢T Y√äU C·∫¶U ‚â•3)")

# B∆∞·ªõc 1: Missing Values
with st.expander("1Ô∏è‚É£ **Missing Values** - X·ª≠ l√Ω gi√° tr·ªã thi·∫øu", expanded=True):
    col1, col2 = st.columns([1, 2])
    
    with col1:
        st.markdown("""
        #### üìù M√¥ t·∫£ v·∫•n ƒë·ªÅ
        - **Year c√≥ missing**: Phim kh√¥ng c√≥ nƒÉm ph√°t h√†nh
        - **Genres c√≥ missing**: Phim kh√¥ng c√≥ th·ªÉ lo·∫°i
        
        #### üîß Gi·∫£i ph√°p
        - Fill NaN v·ªõi 'Unknown'
        - Ho·∫∑c l·∫•y gi√° tr·ªã t·ª´ ngu·ªìn kh√°c
        """)
    
    with col2:
        st.code("""# X·ª≠ l√Ω missing values
# Ki·ªÉm tra missing values
print("Missing values tr∆∞·ªõc khi x·ª≠ l√Ω:")
print(f"Year: {movies['year'].isnull().sum()}")
print(f"Genres: {movies['genres'].isnull().sum()}")

# X·ª≠ l√Ω missing values
movies['year'] = movies['year'].fillna('Unknown')
movies['genres'] = movies['genres'].fillna('(no genres listed)')

print("\nSau khi x·ª≠ l√Ω:")
print(f"Year: {movies['year'].isnull().sum()}")
print(f"Genres: {movies['genres'].isnull().sum()}")
""", language="python")
        
        # Demo k·∫øt qu·∫£
        if st.button("‚ñ∂Ô∏è Ch·∫°y demo B∆∞·ªõc 1", key="step1"):
            st.success("‚úÖ ƒê√£ x·ª≠ l√Ω xong missing values!")
            st.write(f"**Tr∆∞·ªõc:** {movies_orig['year'].isnull().sum()} missing trong c·ªôt 'year'")
            st.write(f"**Sau:** {movies_clean['year'].isnull().sum()} missing trong c·ªôt 'year'")

# B∆∞·ªõc 2: Lo·∫°i b·ªè Duplicate
with st.expander("2Ô∏è‚É£ **Lo·∫°i b·ªè Duplicate** - X√≥a b·∫£n ghi tr√πng l·∫∑p"):
    col1, col2 = st.columns([1, 2])
    
    with col1:
        st.markdown("""
        #### üìù M√¥ t·∫£ v·∫•n ƒë·ªÅ
        - **Tr√πng movieId**: C√πng phim xu·∫•t hi·ªán nhi·ªÅu l·∫ßn
        - **Tr√πng title + year**: Phim tr√πng t√™n v√† nƒÉm
        
        #### üîß Gi·∫£i ph√°p
        - Drop duplicate theo movieId
        - Gi·ªØ b·∫£n ghi ƒë·∫ßu ti√™n
        """)
    
    with col2:
        st.code("""# Lo·∫°i b·ªè duplicate
# Ki·ªÉm tra duplicate
duplicate_count = movies.duplicated(subset=['movieId']).sum()
print(f"S·ªë b·∫£n ghi tr√πng l·∫∑p theo movieId: {duplicate_count}")

# Lo·∫°i b·ªè duplicate
movies = movies.drop_duplicates(subset=['movieId'], keep='first')

# Ki·ªÉm tra l·∫°i
print(f"S·ªë b·∫£n ghi sau khi lo·∫°i b·ªè duplicate: {len(movies)}")

# Th√¥ng tin kh√°c
print(f"S·ªë phim duy nh·∫•t: {movies['movieId'].nunique()}")
""", language="python")
        
        if st.button("‚ñ∂Ô∏è Ch·∫°y demo B∆∞·ªõc 2", key="step2"):
            duplicates_before = movies_orig.duplicated(subset=['movieId']).sum()
            duplicates_after = movies_clean.duplicated(subset=['movieId']).sum()
            st.success(f"‚úÖ ƒê√£ lo·∫°i b·ªè {duplicates_before} b·∫£n ghi tr√πng l·∫∑p!")
            st.write(f"**Tr∆∞·ªõc:** {duplicates_before} duplicate records")
            st.write(f"**Sau:** {duplicates_after} duplicate records")

# B∆∞·ªõc 3: Chu·∫©n h√≥a D·ªØ li·ªáu
with st.expander("3Ô∏è‚É£ **Chu·∫©n h√≥a D·ªØ li·ªáu** - ƒê·ªãnh d·∫°ng th·ªëng nh·∫•t"):
    col1, col2 = st.columns([1, 2])
    
    with col1:
        st.markdown("""
        #### üìù M√¥ t·∫£ v·∫•n ƒë·ªÅ
        - **Genres format kh√¥ng nh·∫•t qu√°n**: "Action|Adventure" vs "Adventure|Action"
        - **Year format**: "1995" vs "(1995)" vs "1995.0"
        
        #### üîß Gi·∫£i ph√°p
        - Chu·∫©n h√≥a genres separator
        - Extract year t·ª´ title
        """)
    
    with col2:
        st.code("""# Chu·∫©n h√≥a d·ªØ li·ªáu
# 1. Chu·∫©n h√≥a genres: thay '|' b·∫±ng ' '
movies['genres'] = movies['genres'].str.replace('|', ' ', regex=False)

# 2. T·∫°o c·ªôt content cho TF-IDF
movies['content'] = movies['title'].fillna('') + ' ' + movies['genres'].fillna('')

# 3. Extract year t·ª´ title n·∫øu c·∫ßn
import re

def extract_year(title):
    match = re.search(r'\((\d{4})\)', str(title))
    if match:
        return match.group(1)
    return None

# √Åp d·ª•ng h√†m
movies['year_extracted'] = movies['title'].apply(extract_year)
print("Chu·∫©n h√≥a genres v√† extract year ho√†n t·∫•t!")
""", language="python")
        
        if st.button("‚ñ∂Ô∏è Ch·∫°y demo B∆∞·ªõc 3", key="step3"):
            st.success("‚úÖ ƒê√£ chu·∫©n h√≥a d·ªØ li·ªáu!")
            st.write("**V√≠ d·ª• genres tr∆∞·ªõc:** 'Action|Adventure|Sci-Fi'")
            st.write("**V√≠ d·ª• genres sau:** 'Action Adventure Sci-Fi'")
            st.write("**V√≠ d·ª• content:** 'Toy Story (1995) Adventure Animation Children'")

# B∆∞·ªõc 4: X·ª≠ l√Ω Outlier
with st.expander("4Ô∏è‚É£ **X·ª≠ l√Ω Outlier** - Lo·∫°i b·ªè gi√° tr·ªã ngo·∫°i lai"):
    col1, col2 = st.columns([1, 2])
    
    with col1:
        st.markdown("""
        #### üìù M√¥ t·∫£ v·∫•n ƒë·ªÅ
        - **Rating count qu√° th·∫•p**: Phim c√≥ < 10 rating ‚Üí kh√¥ng ƒë·∫°i di·ªán
        - **Year kh√¥ng h·ª£p l·ªá**: NƒÉm < 1900 ho·∫∑c > 2024
        
        #### üîß Gi·∫£i ph√°p
        - L·ªçc phim c√≥ rating_count > 100
        - L·ªçc nƒÉm h·ª£p l√Ω (1900-2024)
        """)
    
    with col2:
        st.code("""# X·ª≠ l√Ω outlier
# Ki·ªÉm tra outlier trong rating_count
print(f"Rating count - Min: {movies['rating_count'].min()}")
print(f"Rating count - Max: {movies['rating_count'].max()}")
print(f"Phim c√≥ rating_count < 100: {(movies['rating_count'] < 100).sum()}")

# X·ª≠ l√Ω outlier: ch·ªâ gi·ªØ phim ph·ªï bi·∫øn
movies = movies[movies['rating_count'] > 100]

print(f"\nSau khi l·ªçc:")
print(f"S·ªë phim c√≤n l·∫°i: {len(movies)}")
print(f"Rating count - Min: {movies['rating_count'].min()}")
print(f"Rating count - Max: {movies['rating_count'].max()}")

# Ki·ªÉm tra outlier trong nƒÉm
movies = movies[(movies['year'] >= 1900) & (movies['year'] <= 2024)]
""", language="python")
        
        if st.button("‚ñ∂Ô∏è Ch·∫°y demo B∆∞·ªõc 4", key="step4"):
            # T·∫°o bi·ªÉu ƒë·ªì minh h·ªça
            fig, ax = plt.subplots(1, 2, figsize=(10, 4))
            
            # Bi·ªÉu ƒë·ªì rating_count
            ax[0].hist(movies_clean['rating_count'], bins=50, alpha=0.7, color='skyblue')
            ax[0].axvline(x=100, color='red', linestyle='--', label='Ng∆∞·ª°ng 100')
            ax[0].set_xlabel('Rating Count')
            ax[0].set_ylabel('S·ªë phim')
            ax[0].set_title('Ph√¢n ph·ªëi Rating Count')
            ax[0].legend()
            
            # Bi·ªÉu ƒë·ªì nƒÉm
            ax[1].hist(movies_clean['year'].dropna(), bins=30, alpha=0.7, color='lightgreen')
            ax[1].set_xlabel('NƒÉm')
            ax[1].set_ylabel('S·ªë phim')
            ax[1].set_title('Ph√¢n ph·ªëi NƒÉm Ph√°t h√†nh')
            
            plt.tight_layout()
            st.pyplot(fig)
            st.success("‚úÖ ƒê√£ lo·∫°i b·ªè outlier!")

# B∆∞·ªõc 5: Vector h√≥a (TF-IDF)
with st.expander("5Ô∏è‚É£ **Vector h√≥a** - TF-IDF cho Content-Based"):
    col1, col2 = st.columns([1, 2])
    
    with col1:
        st.markdown("""
        #### üìù M√¥ t·∫£ v·∫•n ƒë·ªÅ
        - **Text data kh√¥ng th·ªÉ t√≠nh to√°n tr·ª±c ti·∫øp**
        - **C·∫ßn chuy·ªÉn th√†nh s·ªë** ƒë·ªÉ t√≠nh similarity
        
        #### üîß Gi·∫£i ph√°p
        - TF-IDF Vectorization
        - 10,000 features t·ªëi ƒëa
        - Lo·∫°i b·ªè stopwords ti·∫øng Anh
        """)
    
    with col2:
        st.code("""# Vector h√≥a v·ªõi TF-IDF
from sklearn.feature_extraction.text import TfidfVectorizer
import pickle

# Kh·ªüi t·∫°o TF-IDF Vectorizer
tfidf = TfidfVectorizer(
    stop_words='english',      # Lo·∫°i b·ªè stopwords
    max_features=10000,        # Gi·ªõi h·∫°n s·ªë features
    ngram_range=(1, 2)         # X√©t 1-2 t·ª´
)

# √Åp d·ª•ng TF-IDF
tfidf_matrix = tfidf.fit_transform(movies['content'])

print(f"Shape c·ªßa TF-IDF matrix: {tfidf_matrix.shape}")
print(f"S·ªë t·ª´ v·ª±ng: {len(tfidf.get_feature_names_out())}")

# T√≠nh Cosine Similarity
from sklearn.metrics.pairwise import cosine_similarity
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

print(f"Shape c·ªßa Cosine Similarity matrix: {cosine_sim.shape}")

# L∆∞u model ƒë·ªÉ s·ª≠ d·ª•ng sau
with open('data/cosine_sim.pkl', 'wb') as f:
    pickle.dump(cosine_sim, f)

print("‚úÖ Vector h√≥a ho√†n t·∫•t v√† ƒë√£ l∆∞u model!")
""", language="python")
        
        if st.button("‚ñ∂Ô∏è Ch·∫°y demo B∆∞·ªõc 5", key="step5"):
            try:
                # Demo TF-IDF
                sample_texts = movies_clean['content'].head(5).tolist()
                demo_tfidf = TfidfVectorizer(max_features=20)
                demo_matrix = demo_tfidf.fit_transform(sample_texts)
                
                st.success("‚úÖ Vector h√≥a th√†nh c√¥ng!")
                st.write(f"**Shape TF-IDF matrix:** {demo_matrix.shape}")
                st.write(f"**V√≠ d·ª• t·ª´ v·ª±ng:** {demo_tfidf.get_feature_names_out()[:10]}")
                
                # Hi·ªÉn th·ªã ma tr·∫≠n TF-IDF nh·ªè
                st.write("**Ma tr·∫≠n TF-IDF (5 phim x 20 features):**")
                st.dataframe(
                    pd.DataFrame(
                        demo_matrix.toarray(),
                        columns=demo_tfidf.get_feature_names_out(),
                        index=movies_clean['title'].head(5)
                    ),
                    use_container_width=True
                )
            except Exception as e:
                st.error(f"L·ªói: {e}")

# ========== T·ªîNG K·∫æT ==========
st.markdown("---")
st.markdown("### üéØ T·ªîNG K·∫æT QUY TR√åNH L√ÄM S·∫†CH")

# T·∫°o b·∫£ng t·ªïng k·∫øt
summary_data = {
    "B∆∞·ªõc": ["1. Missing Values", "2. Lo·∫°i b·ªè Duplicate", "3. Chu·∫©n h√≥a D·ªØ li·ªáu", "4. X·ª≠ l√Ω Outlier", "5. Vector h√≥a"],
    "Ph∆∞∆°ng ph√°p": ["Fill v·ªõi 'Unknown'", "drop_duplicates()", "replace() + regex", "L·ªçc theo ng∆∞·ª°ng", "TF-IDF Vectorizer"],
    "K·∫øt qu·∫£": [
        "Kh√¥ng c√≤n NaN values",
        "Kh√¥ng c√≤n tr√πng l·∫∑p",
        "D·ªØ li·ªáu nh·∫•t qu√°n",
        "D·ªØ li·ªáu ch·∫•t l∆∞·ª£ng cao",
        "S·∫µn s√†ng cho ML"
    ],
    "Tr·∫°ng th√°i": ["‚úÖ Ho√†n th√†nh", "‚úÖ Ho√†n th√†nh", "‚úÖ Ho√†n th√†nh", "‚úÖ Ho√†n th√†nh", "‚úÖ Ho√†n th√†nh"]
}

summary_df = pd.DataFrame(summary_data)
st.dataframe(
    summary_df,
    use_container_width=True,
    hide_index=True,
    column_config={
        "B∆∞·ªõc": st.column_config.TextColumn(width="large"),
        "Ph∆∞∆°ng ph√°p": st.column_config.TextColumn(width="medium"),
        "K·∫øt qu·∫£": st.column_config.TextColumn(width="medium"),
        "Tr·∫°ng th√°i": st.column_config.Column(
            width="small",
            help="Tr·∫°ng th√°i ho√†n th√†nh"
        )
    }
)

# Th√†nh t·ª±u
st.success("""
### üèÜ TH√ÄNH T·ª∞U ƒê·∫†T ƒê∆Ø·ª¢C

‚úì **D·ªØ li·ªáu s·∫°ch 100%**: Kh√¥ng missing, kh√¥ng duplicate  
‚úì **Chu·∫©n h√≥a ho√†n to√†n**: D·ªØ li·ªáu nh·∫•t qu√°n  
‚úì **Vector h√≥a th√†nh c√¥ng**: S·∫µn s√†ng cho machine learning  
‚úì **Optimized for recommendation**: T·ªëi ∆∞u cho h·ªá th·ªëng g·ª£i √Ω
""")

# ========== DOWNLOAD D·ªÆ LI·ªÜU ƒê√É L√ÄM S·∫†CH ==========
st.markdown("---")
st.markdown("### üì• T·∫¢I D·ªÆ LI·ªÜU ƒê√É L√ÄM S·∫†CH")

@st.cache_data
def convert_to_csv(df):
    return df.to_csv(index=False).encode('utf-8')

csv_data = convert_to_csv(movies_clean)

col_dl1, col_dl2 = st.columns(2)
with col_dl1:
    st.markdown("T·∫£i to√†n b·ªô dataset ƒë√£ l√†m s·∫°ch:")
with col_dl2:
    st.download_button(
        label="üì• movies_final_cleaned.csv",
        data=csv_data,
        file_name="movies_final_cleaned.csv",
        mime="text/csv",
        use_container_width=True
    )

# ========== CHUY·ªÇN TI·∫æP ==========
st.markdown("---")
st.info("""
**üìä D·ªØ li·ªáu ƒë√£ s·∫µn s√†ng cho b∆∞·ªõc ti·∫øp theo: PH√ÇN T√çCH & TR·ª∞C QUAN H√ìA**

üëâ S·ª≠ d·ª•ng menu b√™n tr√°i ƒë·ªÉ chuy·ªÉn sang trang 3
""")